import pandas as pd
import numpy as np
import xgboost as xgb
import joblib
import warnings
import os
from pathlib import Path
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error

warnings.filterwarnings('ignore')


#  H√ÄU
OYSTER_FEATURES = [
    'DO', 'Temperature', 'pH', 'Salinity', 'NH3', 'H2S', 'BOD5', 'COD',
    'TSS', 'Coliform', 'Alkalinity', 'Transparency',
]

#  C√Å GI√í
COBIA_FEATURES = [
    'DO', 'Temperature', 'pH', 'Salinity', 'NH3', 'PO4', 'BOD5', 'COD',
    'TSS', 'Coliform', 'Alkalinity', 'Transparency'
]


# H√†m ƒë·ªçc CSV, l√†m s·∫°ch, ƒëi·ªÅn d·ªØ li·ªáu thi·∫øu v√† t·∫°o Lag Features.
def prepare_time_series_data(csv_path, features_list, lags=[1, 4]):
    csv_path = str(csv_path)
    df = pd.read_csv(csv_path)
    
    # Ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt c·∫ßn thi·∫øt
    keep_cols = ['Station', 'Quarter'] + [f for f in features_list if f in df.columns]
    df = df[keep_cols]

    # X·ª≠ l√Ω th·ªùi gian (ƒë·ªïi th√†nh th·ªùi gian theo qu√Ω)
    df['Date'] = pd.to_datetime(df['Quarter'], errors='coerce')
    df = df.dropna(subset=['Date'])
    df = df.sort_values(by=['Station', 'Date'])

    # ƒêi·ªÅn d·ªØ li·ªáu thi·∫øu (Imputation) theo tr·∫°m (m·ªôt s·ªë tr·∫°m b·ªã thi·∫øu qu√Ω, v√≠ d·ª• thi·∫øu 2020Q2 th√¨ l·∫•y trung b√¨nh c·ªßa Q1 v√† Q3)
    def fill_missing(group):
        # N·ªôi suy tuy·∫øn t√≠nh
        group[features_list] = group[features_list].interpolate(method='linear', limit_direction='both')
        # Fill median tr·∫°m
        group[features_list] = group[features_list].fillna(group[features_list].median())
        return group

    df = df.groupby('Station').apply(fill_missing).reset_index(drop=True)
    
    # Fill median cho c√°c √¥ feature b·ªã thi·∫øu
    df[features_list] = df[features_list].fillna(df[features_list].median())

    # T·∫°o Lag Features
    lag_cols = []
    for col in features_list:
        for lag in lags:
            new_col_name = f"{col}_lag{lag}"
            lag_cols.append(new_col_name)
            df[new_col_name] = df.groupby('Station')[col].shift(lag)
    
    df['Quarter_Num'] = df['Date'].dt.quarter
    time_features = ['Quarter_Num']

    df_final = df.dropna().reset_index(drop=True)
    
    print(f"K√≠ch th∆∞·ªõc d·ªØ li·ªáu train: {df_final.shape}")
    
    input_features = lag_cols + time_features
    return df_final, input_features

# H√†m x·ª≠ l√Ω ngo·∫°i l·ªá
def clip_percentile(series, lower=0.01, upper=0.99):
    lo = series.quantile(lower)
    hi = series.quantile(upper)
    return series.clip(lo, hi)

def handle_outliers(df, features):
    df = df.copy()

    log_cols = ["Coliform", "TSS", "BOD5", "NH3"]
    for c in log_cols:
        if c in features and c in df.columns:
            df[c] = clip_percentile(df[c], 0.01, 0.99)

    return df


# H√†m hu·∫•n luy·ªán
def train_forecast_model(csv_path, features, model_out_path, meta_out_path=None):
    model_out_path = str(model_out_path)
    
    df_train, input_cols = prepare_time_series_data(csv_path, features, lags=[1, 4])
    
    if df_train is None:
        return
    
    df_train = handle_outliers(df_train, features)

    X = df_train[input_cols]      # Qu√° kh·ª©
    y = df_train[features]        # Hi·ªán t·∫°i (M·ª•c ti√™u)

    # C√°c tham s·ªë
    model = MultiOutputRegressor(xgb.XGBRegressor(
        n_estimators=1000,
        learning_rate=0.05,
        max_depth=5,            # ƒê·ªô s√¢u trung b√¨nh (tr√°nh overfit)
        subsample=0.8,          # M·ªói c√¢y h·ªçc 80% s·ªë d√≤ng
        colsample_bytree=0.8,   # M·ªói c√¢y h·ªçc 80% s·ªë c·ªôt, gi·ªëng ki·ªÉu drop out trong NN
        objective='reg:squarederror',
        n_jobs=-1,
        random_state=42
    ))

    model.fit(X, y)
    
    # T√≠nh RMSE sau khi train (d√πng t·∫≠p train ƒë·ªÉ test n√™n l√† k·∫øt qu·∫£ ko c√≥ √Ω nghƒ©a l·∫Øm)
    print("\nüìä K·∫æT QU·∫¢ ƒê√ÅNH GI√Å (TRAINING SCORE):")
    print("-" * 50)
    
    y_pred = model.predict(X)
    
    mse = mean_squared_error(y, y_pred, multioutput='raw_values')
    rmse = np.sqrt(mse)
    
    for i, col_name in enumerate(features):
        print(f"   üîπ {col_name:<15} RMSE: {rmse[i]:.4f}")
        
    print("-" * 50)
    print(f"üëâ RMSE trung b√¨nh to√†n m√¥ h√¨nh: {np.mean(rmse):.4f}")


    # L∆∞u model
    joblib.dump(model, model_out_path)
    print(f"\nüéâ ƒê√£ l∆∞u model t·∫°i: {model_out_path}")

    # L∆∞u metadata
    if meta_out_path is None:
        meta_out_path = model_out_path.replace('.pkl', '_features.pkl')
    
    joblib.dump((input_cols, features), meta_out_path)
    print(f"‚ÑπÔ∏è  ƒê√£ l∆∞u danh s√°ch features t·∫°i: {meta_out_path}")


if __name__ == "__main__":

    BASE_DIR = Path(__file__).resolve().parent
    PROJECT_DIR = BASE_DIR.parent 
    
    OUTPUT_DIR = PROJECT_DIR / "model" / "output"
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    DATA_DIR = PROJECT_DIR / "data" / "hk_water_quality"


    train_forecast_model(
        csv_path = DATA_DIR / "hk_oyster_quarterly_21vars.csv",
        features = OYSTER_FEATURES,
        model_out_path = OUTPUT_DIR / "hk_oyster_forecast_model.pkl"
    )


    train_forecast_model(
        csv_path = DATA_DIR / "hk_cobia_quarterly_21vars.csv",
        features = COBIA_FEATURES,
        model_out_path = OUTPUT_DIR / "hk_cobia_forecast_model.pkl"
    )